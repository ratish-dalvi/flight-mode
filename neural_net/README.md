Simple Neural Network implementation from Scratch
-------------------------------------------------

This is a basic implementation of a Neural Network from scratch, using numpy arrays. 

It implements basic stochastic gradient descent with fixed eta, and a simple version of back propagation with quadratic loss function. 

This code is for demonstration/learning purposes only. It is very inefficient and is not meant for training.

I tested this with MNIST, and it gets 90%+ in a few minutes on a consumer-grade laptop

I wrote this code when I was bored on a long 14 hour flight to India, and it was all written from memory and my basic understanding of neural networks; apologies if you find it hacky and buggy. 


Test
----

``` python
python test_mnist.py
```

