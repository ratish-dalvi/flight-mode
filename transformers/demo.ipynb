{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc7a931",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e28b7",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7ae550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from trainer import DEFAULT_CONFIG as config\n",
    "from transformer import Transformer\n",
    "\n",
    "\n",
    "MODEL_PATH = \"./model_output/checkpoint-120000/pytorch_model.bin\"\n",
    "COMPARE_TO_GPT = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_custom = Transformer(\n",
    "    config['embedding_size'],\n",
    "    tokenizer.vocab_size,\n",
    "    config['context_length'],\n",
    "    config['num_layers'],\n",
    "    config['dropout'],\n",
    "    config['mult'],\n",
    "    config['num_heads'],\n",
    "    device\n",
    ")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "model_custom.load_state_dict(state_dict)\n",
    "model_custom.to(device)\n",
    "\n",
    "\n",
    "if COMPARE_TO_GPT:\n",
    "    model_gpt = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "    model_gpt.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5843d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      "Custom model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the united states is a new way of making the country a better place for the country.\n",
      "\n",
      "The president of the United States, who is the first president of the\n",
      "\n",
      "GPT model: \n",
      "The president of the united states is the head of the executive branch of government. The president is the head of the executive branch of government.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Custom model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, I was a little kid. I was a kid. I was a kid. I was a kid. I was a kid. I was a\n",
      "\n",
      "GPT model: \n",
      "Once upon a time, the United States was the world's leading producer of oil and natural gas. Today, we are the world's leading producer of\n",
      "==================================================\n",
      "\n",
      "Custom model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about the world is that it is the most important thing about the world.\n",
      "\n",
      "The world is the most important thing about the world. It is the\n",
      "\n",
      "GPT model: \n",
      "The best thing about the new system is that it's easy to use. You can use it to create a new account, or you can use it\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def generate_text(prompt, max_tokens=30, temperature=1, model=None):\n",
    "    \"\"\"\n",
    "    Generate text given a prompt\n",
    "    \"\"\"\n",
    "    # Encode the initial text\n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    st = time.time()\n",
    "    # Generate additional tokens\n",
    "    generated_ids = model.generate(input_ids, max_tokens, temperature=temperature)\n",
    "    # print(f\"Text generated at {generated_ids.shape[1]/(time.time() - st):.1f} tokens/second\")\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(\n",
    "        generated_ids[0], skip_special_tokens=True\n",
    "    )\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"The president of the united states is\",\n",
    "    \"Once upon a time\",\n",
    "    \"The best thing about\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nCustom model: \")\n",
    "    print(generate_text(prompt, model=model_custom))\n",
    "    \n",
    "    if COMPARE_TO_GPT:\n",
    "        print(\"\\nGPT model: \")\n",
    "        print(generate_text(prompt, model=model_gpt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f20303",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise  # don't run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feda2661",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(out)\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m model_gpt\n\u001b[0;32m---> 34\u001b[0m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOnce upon a time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, num_samples, steps, do_sample)\u001b[0m\n\u001b[1;32m     12\u001b[0m         x \u001b[38;5;241m=\u001b[39m tokenizer(prompt)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_type)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prompt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: \n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# to create unconditional samples...\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# huggingface/transformers tokenizer special cases these strings\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
    "        \n",
    "    # tokenize the input prompt into integer input sequence\n",
    "    if 0:\n",
    "        tokenizer = BPETokenizer()\n",
    "        if prompt == '':\n",
    "            # to create unconditional samples...\n",
    "            # manually create a tensor with only the special <|endoftext|> token\n",
    "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
    "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "        else:\n",
    "            x = tokenizer(prompt).to(device)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "        if prompt == '': \n",
    "            # to create unconditional samples...\n",
    "            # huggingface/transformers tokenizer special cases these strings\n",
    "            prompt = '<|endoftext|>'\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        x = encoded_input['input_ids']\n",
    "    \n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('-'*80)\n",
    "        print(out)\n",
    "        \n",
    "model = model_gpt\n",
    "generate(prompt='Once upon a time', num_samples=10, steps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
